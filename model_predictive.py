# -*- coding: utf-8 -*-
"""Model_predictive.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1biaQFYNGHkvvWp3wBGzyWaskljDcLOsy

# Prediksi Hujan di Australia

## Latar Belakang
Cuaca yang sulit diprediksi, khususnya hujan, memengaruhi banyak sektor seperti pertanian, transportasi, dan perencanaan perkotaan. Di Australia, perubahan cuaca yang ekstrem sering menyebabkan banjir atau kekeringan, berdampak pada kehidupan masyarakat dan aktivitas bisnis. Prediksi hujan sehari sebelumnya dapat membantu perencanaan yang lebih baik dalam berbagai bidang yang terdampak oleh kondisi cuaca.

## Mengapa dan Bagaimana Masalah Ini Harus Diselesaikan
Memecahkan masalah prediksi hujan harian penting karena:
1. Mengurangi risiko operasional dan biaya dalam sektor pertanian dan transportasi.
2. Meminimalkan dampak cuaca ekstrem dengan membantu masyarakat dan organisasi dalam membuat keputusan berdasarkan data.

Dengan memanfaatkan data historis cuaca, model pembelajaran mesin dapat dibangun untuk memprediksi apakah akan turun hujan keesokan harinya. Algoritma seperti *Random Forest*, *Logistic Regression*, atau *Gradient Boosting* dapat digunakan untuk melatih model ini. Teknik *feature engineering* yang relevan, seperti distribusi suhu, tekanan, dan kelembaban, penting untuk meningkatkan akurasi prediksi.

## Hasil Riset Terkait
Beberapa studi mendukung pendekatan ini:
- *Agrawal et al.* (2020) menunjukkan bahwa algoritma seperti *Random Forest* dan *XGBoost* mampu mencapai akurasi prediksi hingga 80% untuk prediksi hujan harian.
- *Hong et al.* (2018) menekankan pentingnya pemilihan fitur yang tepat untuk meningkatkan akurasi dan mencegah overfitting pada data cuaca yang kompleks.

**Referensi:**
1. Animas, A., et al. (2022). "Rainfall prediction: A comparative analysis of modern machine learning algorithms for time-series forecasting."
2. Malathi Thangavel. (2020). "FEATURE SELECTION TECHNIQUES FOR WEATHER FORECASTING MODELS USING MACHINE LEARNING TECHNIQUES."

# Business Understanding

## Problem Statement
Perubahan cuaca yang ekstrem di Australia, khususnya hujan yang sulit diprediksi, mempengaruhi berbagai sektor yang bergantung pada kondisi cuaca, seperti pertanian, transportasi, dan infrastruktur. Ketidakpastian dalam pola hujan mengakibatkan risiko kerugian ekonomi dan keamanan, seperti banjir yang tiba-tiba dan kekeringan berkepanjangan. Mampu memprediksi hujan sehari sebelumnya akan membantu sektor-sektor ini dalam melakukan perencanaan yang lebih baik, mengurangi risiko, dan meningkatkan efisiensi.

## Goals
Tujuan dari proyek ini adalah membangun model prediksi untuk mengidentifikasi kemungkinan terjadinya hujan keesokan harinya berdasarkan data cuaca harian. Model ini diharapkan memiliki akurasi yang cukup untuk digunakan dalam pengambilan keputusan oleh berbagai pihak yang membutuhkan prediksi cuaca. Model prediksi yang akurat akan memberikan manfaat, seperti:
1. Mengurangi risiko kerugian dalam sektor pertanian melalui perencanaan penyiraman dan panen yang lebih baik.
2. Meningkatkan keamanan dan efisiensi dalam sektor transportasi dan infrastruktur.
3. Menyediakan informasi yang dapat diandalkan untuk perencanaan perkotaan dan mitigasi risiko bencana.

## Solution Statement
Untuk mencapai tujuan di atas, beberapa solusi dapat diterapkan:

1. **Modeling dengan Algoritma Pembelajaran Mesin**
   - Menggunakan dua atau lebih algoritma untuk membangun model prediksi, seperti *Logistic Regression* atau *Support Vector Machine*. Masing-masing algoritma memiliki kelebihan dalam menangani data cuaca yang kompleks, dan hasilnya dapat dibandingkan untuk memilih model terbaik.
   - Metrik evaluasi: Akurasi, Precision, Recall, dan F1-Score.

2. **Improvement pada Dataset dengan Metode Oversampling**
   - Mengatasi masalah ketidakseimbangan kelas dalam data menggunakan metode *oversampling* dengan *SMOTE (Synthetic Minority Over-sampling Technique)* dan *class_weight* pada model. *SMOTE* menghasilkan sampel baru dari kelas minoritas, sedangkan *class_weight* membantu model untuk lebih mempertimbangkan kelas yang kurang dominan. Kedua metode ini bertujuan meningkatkan kemampuan model dalam memprediksi kelas hujan yang langka.
   - Metrik evaluasi: Akurasi, Precision, Recall, dan F1-Score akan digunakan untuk mengevaluasi perbaikan kinerja model setelah penyeimbangan kelas.

Solusi ini diharapkan memberikan model prediksi yang akurat, dengan evaluasi berbasis metrik yang jelas sehingga performa model dapat terukur secara kuantitatif.

# Dataset "Rain in Australia"

Dataset ini berisi informasi seputar cuaca di Australia dengan kolom-kolom sebagai berikut:

1. **Date:** Tanggal pengamatan cuaca.
2. **Location:** Nama lokasi/tempat pengamatan cuaca.
3. **MinTemp:** Temperatur minimum pada hari itu (dalam derajat Celsius).
4. **MaxTemp:** Temperatur maksimum pada hari itu (dalam derajat Celsius).
5. **Rainfall:** Jumlah curah hujan dalam milimeter pada hari itu.
6. **Evaporation:** Penguapan air dari permukaan tanah dan tanaman pada hari itu (dalam milimeter).
7. **Sunshine:** Jumlah jam sinar matahari pada hari itu.
8. **WindGustDir:** Arah angin saat terjadi angin kencang.
9. **WindGustSpeed:** Kecepatan angin maksimum yang tercatat pada hari itu (dalam kilometer per jam).
10. **WindDir9am:** Arah angin pada pukul 9 pagi.
11. **WindDir3pm:** Arah angin pada pukul 3 sore.
12. **WindSpeed9am:** Kecepatan angin pada pukul 9 pagi (dalam kilometer per jam).
13. **WindSpeed3pm:** Kecepatan angin pada pukul 3 sore (dalam kilometer per jam).
14. **Humidity9am:** Kelembaban udara pada pukul 9 pagi (dalam persen).
15. **Humidity3pm:** Kelembaban udara pada pukul 3 sore (dalam persen).
16. **Pressure9am:** Tekanan udara pada pukul 9 pagi (dalam hectopascals).
17. **Pressure3pm:** Tekanan udara pada pukul 3 sore (dalam hectopascals).
18. **Cloud9am:** Jumlah awan pada pukul 9 pagi (dalam okta, 0-8).
19. **Cloud3pm:** Jumlah awan pada pukul 3 sore (dalam okta, 0-8).
20. **Temp9am:** Temperatur pada pukul 9 pagi (dalam derajat Celsius).
21. **Temp3pm:** Temperatur pada pukul 3 sore (dalam derajat Celsius).
22. **RainToday:** Apakah hujan terjadi hari ini (Yes/No).
23. **RainTomorrow:** Apakah diharapkan hujan besok (Yes/No).

# 1. Import Library

1. **Import Library:**
   - `numpy` dan `pandas` untuk manipulasi data.
   - `warnings` untuk menyembunyikan warning.
   - `seaborn` dan `matplotlib.pyplot` untuk visualisasi data.

2. **Set Option:**
   - `pd.set_option("display.max.columns", None)`: Mengatur opsi pandas agar semua kolom dapat ditampilkan.

3. **Import Model untuk Klasifikasi:**
   - `LogisticRegression` dan `SVC` dari scikit-learn untuk digunakan pada masalah klasifikasi.

4. **Split Dataset dan Standarisasi:**
   - `Winsorizer` dari Feature-Engine untuk mengatasi outlier.
   - `LabelEncoder` untuk mengkodekan variabel kategorikal.
   - `RobustScaler` untuk standarisasi data yang tahan terhadap outlier.
   - `train_test_split` dari scikit-learn untuk membagi dataset menjadi training dan testing sets.

5. **Evaluasi Model:**
   - `accuracy_score`, `classification_report`, dan `confusion_matrix` dari scikit-learn untuk mengevaluasi performa model klasifikasi.
"""

# Import library yang dibutuhkan
import numpy as np
import pandas as pd
import warnings
import pickle
import seaborn as sns
import matplotlib.pyplot as plt

# Menyembunyikan warning
warnings.filterwarnings("ignore")

# Set option agar semua kolom dapat ditampilkan
pd.set_option("display.max.columns", None)

# For Classification Problems
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Split Dataset and Standarize the Datasets
from feature_engine.outliers import Winsorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# Evaluate  Models
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""# 2. Data Loading

1. **Membaca Dataset:**
   - `df = pd.read_csv('../../Dataset/weatherAUS.csv')`: Menggunakan Pandas untuk membaca dataset dari file CSV.

2. **Menampilkan 5 Baris Pertama Dataset:**
   - `df.head()`: Menampilkan 5 baris pertama dari dataset untuk memberikan gambaran awal.

3. **Menampilkan Ukuran Dataset:**
   - `df.shape`: Menampilkan ukuran dataset, yaitu jumlah baris dan kolom.

4. **Menampilkan Informasi Dataset:**
   - `df.info()`: Menampilkan informasi dataset, seperti tipe data dan jumlah nilai yang hilang pada setiap kolom.

5. **Menampilkan Statistik Deskriptif:**
   - `df.describe()`: Menampilkan statistik deskriptif untuk setiap kolom numerik dalam dataset, seperti mean, median, dan lainnya.
"""

# Membaca dataset dari file CSV
df = pd.read_csv('../../Dataset/weatherAUS.csv')

# Menampilkan 5 baris pertama dari dataset
df.head()

# Menampilkan ukuran dataset (jumlah baris dan kolom)
df.shape

# Menampilkan informasi dataset seperti tipe data dan nilai yang hilang
df.info()

# Menampilkan statistik deskriptif untuk setiap kolom numerik dalam dataset
df.describe()

"""# 3. Data Cleaning

1. **RainTomorrow**
    - Karena kolom ini adalah target untuk diprediksi maka nilai kosong yang ada pada kolom ini akan dihapus.

2. **Numeric Columns**
    - Membagi menjadi 3 cara yaitu imputasi menggunakan mean untuk distribusi normal, median untuk distribusi skewed dan random data untuk kolom `Cloud9am` dan `Cloud3pm`.
    - Kolom dipisah sesuai jenis distribusinya kecuali kolom `Cloud9am` dan `Cloud3pm`.
    - Membuat fungsi untuk masing-masing cara imputasinya.

3. **Object Columns**
    - Melakukan imputasi pada nilai `NaN` dengan cara mengisinya dengan mode.

4. **Correlation Columns**
    - Melihat kolom apakah memiliki korelasi terhadap kolom lainnya menggunakan Correlation matrix.
"""

# Menampilkan 5 baris pertama dari dataset
df.head()

# Menampilkan jumlah nilai NaN pada setiap kolom
df.isnull().sum()

# Menghapus baris yang memiliki nilai NaN pada kolom 'RainTomorrow'
df.dropna(subset=['RainTomorrow'], inplace=True)

# Menampilkan informasi dataset setelah penghapusan nilai NaN
df.info()

# Menampilkan jumlah nilai NaN pada setiap kolom setelah penghapusan
df.isnull().sum()

# Memilih kolom numerik yang bukan bertipe objek
numeric_columns = df.select_dtypes(exclude='object').columns

# Memilih kolom numerik yang memiliki nilai NaN
numeric_columns_with_nan = df[numeric_columns].columns[df[numeric_columns].isna().any()].tolist()

# Menghitung skewness dan membuat diagram distribusi
for col in numeric_columns_with_nan:
    plt.figure(figsize=(10, 6))
    sns.histplot(df[col].dropna(), kde=True)
    plt.title(f'Distribution of {col}')

    plt.show()

    skewness = df[col].skew()
    print(f'Skewness of {col}: {skewness}')

df['Cloud9am'].value_counts().sort_index()

"""**NOTE:** <br>
Kolom yang memiliki distribusi Normal : `MinTemp`, `MaxTemp`, `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm`, `Humidity3pm`, `Pressure9am`, `Pressure3pm`, `Cloud9am`, `Cloud3pm`, `Temp9am`, `Temp3pm`. <br><br>
Kolom yang memiliki distribusi skewed : `Rainfall`, `Evaporation` <br> <br>

### Handling NaN Value
- Kolom dengan jenis object akan di imputasi dengan `mode`
- Kolom yang memiliki distribusi `Normal` akan di imputasi dengan `Mean`
- Kolom yang memiliki distribusi `Skewed` akan di imputasi dengan `Median`
- Khusus untuk kolom `Cloud9am` dan `Cloud3pm` akan di imputasi dengan `Random sample`
"""

# Membuat fungsi untuk imputasi kolom object
def impute_object_columns(df):
    # Memilih kolom-kolom bertipe objek
    object_columns = df.select_dtypes(include='object').columns

    # Loop melalui kolom-kolom tersebut dan mengimpute dengan modus
    for column in object_columns:
        mode_value = df[column].mode().iloc[0]  # Mengambil nilai modus
        df[column].fillna(mode_value, inplace=True)  # Mengimpute nilai kosong dengan modus

    return df

# Penggunaan fungsi untuk imputasi kolom objek
df = impute_object_columns(df)

# Menampilkan jumlah nilai NaN setelah imputasi kolom objek
df.isnull().sum()

# Memisahkan kolom numerik yang memiliki distribusi normal dan skewed
numeric_columns_normal = [col for col in numeric_columns_with_nan if col not in ['Cloud3pm', 'Cloud9am', 'Rainfall', 'Evaporation']]
numeric_columns_skewed = numeric_columns_with_nan[2:4]

print(numeric_columns_with_nan)
print(numeric_columns_normal)
print(numeric_columns_skewed)

# Membuat fungsi untuk imputasi kolom numerik dengan mean (normal) dan median (skewed)
def impute_numeric_columns(df, normal_columns, skewed_columns):
    # Imputasi kolom dengan distribusi normal menggunakan mean
    for column in normal_columns:
        mean_value = df[column].mean()
        df[column].fillna(mean_value, inplace=True)

    # Imputasi kolom dengan distribusi skewed menggunakan median
    for column in skewed_columns:
        median_value = df[column].median()
        df[column].fillna(median_value, inplace=True)

    return df

# Penggunaan fungsi untuk imputasi kolom numerik
df = impute_numeric_columns(df, numeric_columns_normal, numeric_columns_skewed)

# Menampilkan jumlah nilai NaN setelah imputasi kolom numerik
df.isnull().sum()

# Membuat fungsi untuk imputasi kolom 'Cloud3pm' dan 'Cloud9am' dengan nilai acak dari kolom tersebut
def impute_random_sample(df, cloud_columns):
    for column in cloud_columns:
        # Menentukan indeks data yang memiliki nilai NaN pada kolom tertentu
        nan_indices = df[df[column].isna()].index

        # Mengambil sample secara acak dari kolom tersebut
        random_sample = df[column].dropna().sample(len(nan_indices), replace=True)

        # Mengisi nilai NaN dengan sample secara acak
        df.loc[nan_indices, column] = random_sample.values

    return df

# Menslicing kolom Cloud3pm dan Cloud9am
cloud_columns = ['Cloud3pm', 'Cloud9am']

# Penggunaan fungsi untuk imputasi kolom dengan nilai acak
df = impute_random_sample(df, cloud_columns)

# Menampilkan jumlah nilai NaN setelah imputasi kolom dengan nilai acak
df.isnull().sum()

df.head()

#The `plot_corr` function is used to create a correlation matrix heatmap plot for a given DataFrame,
#visualizing the pairwise correlations between numeric columns within the dataset.
#It provides a quick and intuitive way to assess the relationships between variables in the data.
def plot_corr(df, size=14):
    corr = df.corr()
    fig, ax = plt.subplots(figsize=(size,size))
    ax.matshow(corr)
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.columns)), corr.columns)

plot_corr(df)

len(df['Location'].value_counts())

df['Date'] = pd.to_datetime(df['Date'])

# Mengatur ukuran plot
plt.figure(figsize=(14, 6))

# Menggunakan line plot
sns.lineplot(x='Date', y='Rainfall', data=df)

# Menambahkan label dan judul
plt.title('Rainfall Over Time')
plt.xlabel('Date')
plt.ylabel('Rainfall (mm)')

# Menampilkan plot
plt.show()

"""**NOTE:** <br>
Untuk saat ini semua kolom yang ada pada dataset sangat penting untuk pembuatan model kecuali Date karena tujuan model untuk memprediksi hujan pada esok hari.
"""

# menghapus kolom date
df = df.drop(columns='Date')
df.sample()

"""# 4. EXPLORASI DATA"""

# Mengurutkan lokasi berdasarkan rata-rata curah hujan tertinggi
top_rain_locations = df.groupby('Location')['Rainfall'].mean().sort_values(ascending=False).head(10)

# Plotting
plt.figure(figsize=(12, 6))
sns.barplot(x=top_rain_locations, y=top_rain_locations.index, palette='viridis')
plt.title('Top 10 Locations with Highest Rainfall')
plt.xlabel('Total Rainfall (mm)')
plt.ylabel('Location')
plt.show()

# Countplot untuk RainTomorrow
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='RainTomorrow', palette='pastel')
plt.title('Distribution of Rain Tomorrow')
plt.xlabel('Rain Tomorrow')
plt.ylabel('Count')
plt.show()

"""**KESIMPULAN:** Dari diagram tersebut kita tahu bahwa target yang di prediksi tidak seimbang jumlahnya.

1. **Memilih Kolom Numerik:**
   - `numeric_columns = df.select_dtypes(exclude='object').columns`: Memilih kolom-kolom yang bertipe data numerik.

2. **Menghitung Skewness dan Membuat Diagram Distribusi:**
   - Melakukan eksplorasi data dengan menghitung skewness dan membuat diagram distribusi untuk setiap kolom numerik.
   - Loop melalui kolom numerik, mencetak nilai skewness, dan menampilkan histogram serta boxplot untuk setiap kolom.
   - Digunakan matplotlib dan seaborn untuk visualisasi.
"""

# Memilih kolom numerik yang bukan bertipe objek
numeric_columns = df.select_dtypes(exclude='object').columns
numeric_columns

# Menghitung skewness dan membuat diagram distribusi
for col in numeric_columns:
    skewness = df[col].skew()
    print(f'Skewness of {col}: {skewness}')

    # Define figure size
    plt.figure(figsize=(16, 4))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[col], bins=30)
    plt.title(f'Histogram of {col}')

    # Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')

    plt.show()

"""**KESIMPULAN:** <br>
- Kolom yang memiliki outlier adalah `MinTemp`, `MaxTemp`, `Rainfall`, `Evaporation`, `Sunshine`, `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm`, `Humidity9am`, `Pressure9am`, `Pressure3pm`, `Temp9am`, `Temp3pm`.
- Kolom yang memiliki distribusi Normal adalah `MinTemp`, `MaxTemp`, `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm`, `Humidity3pm`, `Pressure9am`, `Pressure3pm`, `Cloud9am`, `Cloud3pm`, `Temp9am`, `Temp3pm`.
- Kolom yang memiliki distribusi skewed adalah `Rainfall`, `Evaporation`.

# 5. DATA PREPROCESSING

## 5.1 Outlier Handling

1. **Inisialisasi Winsorizer untuk Distribusi Gaussian:**
   - `handling_outlier_gaussian`: Menggunakan Winsorizer dengan metode distribusi gaussian untuk beberapa kolom numerik tertentu.
   - `fold=3`: Menerapkan Winsorizing dengan menggantikan nilai outlier yang melebihi batas tiga kali deviasi standar.

2. **Handling Outlier dengan Distribusi Gaussian:**
   - Proses handling outlier dengan menggunakan Winsorizer pada distribusi gaussian.
   - Membuat boxplot sebelum dan setelah handling outlier untuk kolom 'Pressure3pm' sebagai contoh.

3. **Inisialisasi Winsorizer untuk Distribusi IQR:**
   - `handling_outlier_skewed`: Menggunakan Winsorizer dengan metode distribusi IQR (Interquartile Range) untuk beberapa kolom numerik tertentu.

4. **Handling Outlier dengan Distribusi IQR:**
   - Proses handling outlier dengan menggunakan Winsorizer pada distribusi IQR.
   - Membuat boxplot sebelum dan setelah handling outlier untuk kolom 'Rainfall' sebagai contoh.
"""

df.head()

# Initialize Winsorizer for gaussian distribution
handling_outlier_gaussian = Winsorizer(capping_method='gaussian', # choose gaussian for mean and std
                          tail='both', # cap left, right or both tails
                          fold=3,
                          variables=['MinTemp', 'MaxTemp', 'Sunshine', 'WindGustSpeed',
                                     'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Pressure9am', 'Pressure3pm',
                                     'Temp9am', 'Temp3pm'])

# perform outlier handling
handling_outlier_gaussian.fit(df)

df_ho = handling_outlier_gaussian.transform(df)

# Visualisasi Boxplot sebelum dan sesudah handling outlier
plt.figure(figsize=(10, 4))

# Boxplot MinTemp
plt.subplot(1, 2, 1)
sns.boxplot(y=df['Pressure3pm'])
plt.title('Boxplot of Pressure3pm before handling outlier')

# Boxplot Rainfall
plt.subplot(1, 2, 2)
sns.boxplot(y=df_ho['Pressure3pm'])
plt.title('Boxplot of Pressure3pm After handling outlier')

plt.show()

# Initialize Winsorizer for gaussian distribution
handling_outlier_skewed = Winsorizer(capping_method='iqr', # choose iqr for IQR rule boundaries
                          tail='both', # cap left, right or both tails
                          fold=3,
                          variables=['Rainfall', 'Evaporation'])

# perform outlier handling
handling_outlier_skewed.fit(df_ho)

df_ho = handling_outlier_skewed.transform(df_ho)

plt.figure(figsize=(10, 4))

    # Boxplot MinTemp
    plt.subplot(1, 2, 1)
    sns.boxplot(y=df['Rainfall'])
    plt.title('Boxplot of Rainfall before handling outlier')

    # Boxplot Rainfall
    plt.subplot(1, 2, 2)
    sns.boxplot(y=df_ho['Rainfall'])
    plt.title('Boxplot of Rainfall After handling outlier')

    plt.show()

"""**NOTE**<br>
Pada kolom df_ho masih terdapat outlier setelah di handling yang berspekulasi bahwa cuaca dapat berubah secara tiba-tiba dan ekstrem.
"""

# Print data summaries, generate diagnostic plots, and calculate skewness
print('Dataframe - Before Capping')
print(df.describe())
print('')
print('Dataframe - After Capping')
print(df_ho.describe())

"""## 5.2 Encoding

1. **Menyimpan Nama Kolom Bertipe Objek:**
   - `object_columns = df_ho.select_dtypes(include='object').columns.tolist()`: Menyimpan nama kolom bertipe objek.

2. **Encoding Kolom Kategori menggunakan LabelEncoder:**
   - `label_encoder = LabelEncoder()`: Inisialisasi objek LabelEncoder.
   - Melakukan iterasi pada kolom-kolom bertipe objek dan melakukan encoding.
   - Menyimpan dataset yang sudah diencode ke dalam variabel `df_encoded`.
"""

# Menampilkan 5 baris pertama dari dataset
df_ho.head()

# Menampilkan informasi tentang dataset
df_ho.info()

# Menampilkan jumlah nilai unik pada kolom 'Location'
len(df_ho['Location'].value_counts())

# Menampilkan jumlah nilai unik pada kolom 'WindGustDir'
len(df_ho['WindGustDir'].value_counts())

# Menampilkan jumlah nilai unik pada kolom `WindGustDir', `WindDir9am` dam `WindDir3pm`
df_ho[['WindGustDir', 'WindDir9am', 'WindDir3pm']].value_counts()

# Menyimpan nama kolom bertipe objek
object_columns = df_ho.select_dtypes(include='object').columns.tolist()
object_columns

# calculates the number of unique categories within each of the categorical columns
df_ho[object_columns].nunique()

# Melakukan encoding terhadap kolom kategori
label_encoder = LabelEncoder()
for col in object_columns:
    df_ho[col] = label_encoder.fit_transform(df_ho[col])

df_encoded = df_ho.copy()

df_encoded.head()

# Melihat apakah terdapat nilai Nan pada dataset
df_encoded.isnull().sum().sum()

"""## 5.3 Normalization

1. **Definisi Feature Matrix dan Target Variable:**
   - `fiture = df_encoded.drop(columns='RainTomorrow')`: Mengambil seluruh kolom kecuali 'RainTomorrow' sebagai feature matrix.
   - `target = df_encoded['RainTomorrow']`: Mengambil kolom 'RainTomorrow' sebagai target variable.

2. **Inisialisasi dan Scaling Data menggunakan RobustScaler:**
   - `fiture_scalar = RobustScaler()`: Inisialisasi objek RobustScaler untuk melakukan scaling data.

3. **Melakukan Scaling pada Data Fitur:**
   - `fiture_scalar.fit(fiture)`: Melakukan fitting pada data fitur untuk menghitung median dan IQR.
   - `scaled_fiture = fiture_scalar.transform(fiture)`: Melakukan scaling pada data fitur menggunakan RobustScaler.

4. **Konversi Hasil Scaling Menjadi DataFrame:**
   - `fiture = pd.DataFrame(scaled_fiture, columns=fiture.columns)`: Mengonversi hasil scaling menjadi DataFrame dengan nama kolom yang sesuai.
"""

# Menampilkan sample 1 baris dataset
df_encoded.sample()

# define the feature matrix X and the target variable y
fiture = df_encoded.drop(columns='RainTomorrow')
target = df_encoded['RainTomorrow']

# Initialize the Robust Scaler
fiture_scalar = RobustScaler()

# Scale the Data
fiture_scalar.fit(fiture)

# Melakukan scaling pada data fiture
scaled_fiture = fiture_scalar.transform(fiture)

# Convert to DataFrame
fiture = pd.DataFrame(scaled_fiture, columns=fiture.columns)

# Menampilkan 5 baris pertama dari dataset yang sudah di-scaled
fiture.head()

"""## 5.4 Split Data

Kode ini bertanggung jawab untuk membagi dataset menjadi set pelatihan dan pengujian, langkah dasar dalam pengembangan model machine learning. Pembagian ini memungkinkan pelatihan dan evaluasi model pada set data yang berbeda.

**Penjelasan Kode**:

- **Membagi Data**:
   - Fungsi `train_test_split` dari modul `sklearn.model_selection` digunakan untuk melakukan pembagian data. Fungsi ini memiliki parameter sebagai berikut:
     - `fiture`: Matriks fitur.
     - `target`: Variabel target.
     - `test_size`: Proporsi data yang dialokasikan untuk set pengujian. Dalam kasus ini, diatur menjadi 30% (0,3).
     - `random_state`: Seed untuk generator angka acak untuk memastikan reproduktibilitas.
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(fiture, target, test_size=0.3, random_state=42)

"""- **Output**:
   - Hasil dari pembagian data adalah empat set data:
     - `X_train`: Matriks fitur untuk set pelatihan.
     - `X_test`: Matriks fitur untuk set pengujian.
     - `y_train`: Variabel target untuk set pelatihan.
     - `y_test`: Variabel target untuk set pengujian.

# 6. PENDEFINISIAN MODEL

Kode ini bertanggung jawab untuk mendefinisikan dan melatih dua model machine learning, yaitu Logistic Regression dan Support Vector Machine (SVM).

**Penjelasan Kode**:

1. **Logistic Regression Model**:
   - Model Logistic Regression dibuat menggunakan kelas `LogisticRegression` dari modul `sklearn.linear_model`.
   - Parameter yang digunakan:
     - `solver='liblinear'`: Algoritma solver yang digunakan untuk melatih model.
     - `C=0.001`: Kebalikan dari kekuatan regularisasi; nilai yang lebih rendah mengindikasikan regularisasi yang lebih kuat.
     - `random_state=42`: Seed untuk generator angka acak untuk memastikan reproduktibilitas.
     - `max_iter=100`: Jumlah iterasi maksimum dalam melatih model.

2. **Support Vector Machine (SVM) Model**:
   - Model SVM dibuat menggunakan kelas `SVC` dari modul `sklearn.svm`.
   - Parameter yang digunakan:
     - `kernel='linear'`: Jenis kernel yang digunakan, dalam hal ini, kernel linear.
     - `C=1.0`: Parameter ketegangan yang mengontrol tingkat toleransi terhadap kesalahan pelatihan.
     - `random_state=42`: Seed untuk generator angka acak untuk memastikan reproduktibilitas.
     - `gamma=0.1`: Parameter kernel SVM, gamma.
"""

# Membuat model dan mentraining model machine learning
model_logic = LogisticRegression(solver='liblinear', C=0.001, random_state=42, max_iter=100)

# Membuat model dan mentraining model machine learning
model_svm = SVC(kernel='linear', C=1.0, random_state=42, gamma=0.1)

"""- **Output**:
   - Dua model, `model_logic` dan `model_svm`, siap untuk dilatih dan dievaluasi pada dataset yang sesuai.

# 7. PELATIHAN MODEL

Kode ini bertanggung jawab untuk melatih dua model machine learning yang telah didefinisikan sebelumnya, yaitu model Logistic Regression (`model_logic`) dan model Support Vector Machine (SVM) (`model_svm`).

**Penjelasan Kode**:

1. **Melatih Model Logistic Regression**:
   - Model Logistic Regression (`model_logic`) dilatih menggunakan metode `fit` pada data latih (`X_train` dan `y_train`).
   - Data latih `X_train` adalah matriks fitur, dan `y_train` adalah variabel target.

2. **Melatih Model Support Vector Machine (SVM)**:
   - Model SVM (`model_svm`) juga dilatih menggunakan metode `fit` pada data latih (`X_train` dan `y_train`).
   - Data latih `X_train` adalah matriks fitur, dan `y_train` adalah variabel target.
"""

model_logic.fit(X_train, y_train)

model_svm.fit(X_train, y_train)

"""- **Output**:
   - Dua model, `model_logic` dan `model_svm`, telah dilatih dengan menggunakan data latih dan siap digunakan untuk prediksi pada data uji.

# 8. EVALUASI MODEL

Kode ini melakukan evaluasi terhadap dua model machine learning yang telah dilatih sebelumnya, yaitu model Logistic Regression (`model_logic`) dan model Support Vector Machine (SVM) (`model_svm`).

1. Prediksi dan Akurasi:
   - Model Logistic Regression melakukan prediksi pada data latih (`y_train_pred_logic`) dan data uji (`y_test_pred_logic`).
   - Menghitung akurasi untuk data latih dan data uji menggunakan fungsi `accuracy_score`.
   - Menampilkan laporan klasifikasi (`classification_report`) untuk evaluasi lebih lanjut.

2. Matriks Konfusi:
   - Membuat matriks konfusi untuk hasil prediksi pada data uji menggunakan `confusion_matrix`.
   - Menampilkan matriks konfusi dalam bentuk heatmap.

3. Visualisasi Koefisien:
   - Membuat DataFrame (`coef_df`) untuk menyimpan nilai koefisien dan nama fitur.
   - Mengurutkan DataFrame berdasarkan nilai koefisien.
   - Menampilkan plot bar untuk visualisasi nilai koefisien yang telah diurutkan.
"""

# Melakukan prediksi pada variabel training dan testing
y_train_pred_logic =model_logic.predict(X_train)
y_test_pred_logic = model_logic.predict(X_test)

# Melihat hasil akurasi prediksi
print("Akurasi training: ", accuracy_score(y_train, y_train_pred_logic))
print("Akurasi Uji: ", accuracy_score(y_test, y_test_pred_logic))
print(classification_report(y_test, y_test_pred_logic))

# Buat matriks konfusi dari hasil prediksi testing model machine learning
cm_logic = confusion_matrix(y_test, y_test_pred_logic)
plt.figure(figsize=(6, 6))
sns.heatmap(cm_logic, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Melakukan prediksi pada variabel training dan testing
y_train_pred_svm = model_svm.predict(X_train)
y_test_pred_svm = model_svm.predict(X_test)

# Melihat hasil akurasi prediksi
print("Akurasi training: ", accuracy_score(y_train, y_train_pred_svm))
print("Akurasi Uji: ", accuracy_score(y_test, y_test_pred_svm))
print(classification_report(y_test, y_test_pred_svm))

# Buat matriks konfusi dari hasil prediksi testing model machine learning
cm_svm = confusion_matrix(y_test, y_test_pred_svm)
plt.figure(figsize=(6, 6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Membuat DataFrame untuk menyimpan koefisien dan nama fitur
coef_df = pd.DataFrame({'Feature': fiture.columns, 'Coefficient': abs(model_logic.coef_[0])})

# Mengurutkan DataFrame berdasarkan nilai koefisien secara menurun
coef_df_sorted = coef_df.sort_values(by='Coefficient', ascending=False)

# Plot koefisien yang telah diurutkan
plt.figure(figsize=(12, 6))
sns.barplot(x=coef_df_sorted['Coefficient'], y=coef_df_sorted['Feature'])
plt.title('Logistic Regression Coefficients')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.show()

# Membuat DataFrame untuk menyimpan koefisien dan nama fitur
coef_df = pd.DataFrame({'Feature': fiture.columns, 'Coefficient': abs(model_svm.coef_[0])})

# Mengurutkan DataFrame berdasarkan nilai koefisien secara menurun
coef_df_sorted = coef_df.sort_values(by='Coefficient', ascending=False)

# Plot koefisien yang telah diurutkan
plt.figure(figsize=(12, 6))
sns.barplot(x=coef_df_sorted['Coefficient'], y=coef_df_sorted['Feature'])
plt.title('Linear SVM Coefficients (Sorted)')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.show()

"""**NOTE** <br>
Dari plot diatas terdapat beberapa fitur yang sangat mempengaruhi model logistic regression dan tidak mempengaruhi model SVM dan begitu pun sebaliknya. Maka akan di ambil 10 Fitur yang saling mempengaruhi ke 2 model tersebut dan ditambahkan 1 kolom location untuk memprediksi tempat kota yang di prediksi, diantaranya :
1. 5 Fitur pada Logistic Regression
    - Humidity3pm
    - WindGustSpeed
    - Pressure3pm
    - Cloud3pm
    - Humidity9am

2. 5 Fitur pada SVM
    - Humidity3pm
    - Pressure3pm
    - Pressure9am
    - WindGustSpeed
    - Windspeed3pm

### 11 Fitur yang digunakan
    - Humidity9am
    - Humidity3pm
    - Pressure9am
    - Pressure3pm
    - Cloud3pm
    - WindGustSpeed
    - Windspeed3pm
    - Rainfall
    - Temp3pm
    - RainToday
    - Location

## 8.1 Make a New Fiture

Dari kesimpulan pada grafik sebelumnya maka akan dibuat sebuah fitur baru yang terdiri dari 11 kolom yang diambil dari `df_encoded` dan akan dinormalisasi kembali menggunakan Robust Scalar.
"""

# define the feature
fiture = df_encoded[['Location', 'Humidity3pm', 'Humidity9am', 'Pressure9am', 'Pressure3pm', 'Cloud3pm',
                     'WindGustSpeed', 'WindSpeed3pm', 'Rainfall', 'Temp3pm', 'RainToday']]
fiture.head()

# Mendefinisikan kolom untuk scaling
columns_to_scale = ['Humidity3pm', 'Humidity9am', 'Pressure9am', 'Pressure3pm', 'Cloud3pm',
                    'WindGustSpeed', 'WindSpeed3pm', 'Rainfall', 'Temp3pm']

# Memisahkan kolom 'Location'
location_column = fiture['Location']
rain_column = fiture['RainToday']
fiture = fiture.drop(['Location', 'RainToday'], axis=1)

# Initialize the Robust Scaler
fiture_scalar = RobustScaler()

# Train Scale the Data
fiture_scalar.fit(fiture)

# Melakukan scaling pada kolom-kolom yang ditentukan
features_scaled = fiture_scalar.transform(fiture)

# Membuat DataFrame hasil scaling
features_scaled = pd.DataFrame(features_scaled, columns=columns_to_scale)

# Menambahkan kembali kolom 'Location'
features_scaled['Location'] = location_column.values

# Menambahkan kembali kolom 'Location'
features_scaled['RainToday'] = rain_column.values

# Menampilkan 1 baris features_scaled
features_scaled.sample()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.3, random_state=42)

"""## 8.2 Make a New Model Machine Learning Logisctic Regression

1. **Model Logistic Regression Normal:**
   - Membuat model Logistic Regression (`model_logic_normal`) tanpa menyertakan parameter kelas berimbang.
   - Melatih model menggunakan data latih (`X_train`, `y_train`).
   - Melakukan prediksi pada data latih dan data uji.
   - Menghitung dan menampilkan akurasi serta laporan klasifikasi.

2. **Model Logistic Regression dengan Class Weight 'Balanced':**
   - Membuat model Logistic Regression (`model_logic_balanced`) dengan menyertakan parameter `class_weight='balanced'`.
   - Melatih model menggunakan data latih (`X_train`, `y_train`).
   - Melakukan prediksi pada data latih dan data uji.
   - Menghitung dan menampilkan akurasi serta laporan klasifikasi.

3. **Matriks Konfusi:**
   - Membuat matriks konfusi untuk hasil prediksi pada data uji.
   - Menampilkan matriks konfusi dalam bentuk heatmap.
"""

# Membuat model model machine learning
model_logic_normal = LogisticRegression(random_state=42)

# Menampilkan jumlah sampel sebelum oversampling
print("Jumlah sampel sebelum oversampling:", len(X_train))

# Melakukan oversampling dengan SMOTE
smote = SMOTE(random_state=42)
X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)

# Menampilkan jumlah sampel setelah oversampling
print("Jumlah sampel setelah oversampling:", len(X_train_oversampled))

# mentraining model machine learning
model_logic_normal.fit(X_train_oversampled, y_train_oversampled)

# Melakukan prediksi pada variabel training dan testing
y_train_pred_logic =model_logic_normal.predict(X_train)
y_test_pred_logic = model_logic_normal.predict(X_test)

# Melihat hasil akurasi prediksi
print("Akurasi training: ", accuracy_score(y_train, y_train_pred_logic))
print("Akurasi Uji: ", accuracy_score(y_test, y_test_pred_logic))
print(classification_report(y_test, y_test_pred_logic))

# Buat matriks konfusi dari hasil prediksi testing model machine learning
cm_logic = confusion_matrix(y_test, y_test_pred_logic)
plt.figure(figsize=(6, 6))
sns.heatmap(cm_logic, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Membuat DataFrame untuk menyimpan koefisien dan nama fitur
coef_df = pd.DataFrame({'Feature': features_scaled.columns, 'Coefficient': abs(model_logic_normal.coef_[0])})

# Mengurutkan DataFrame berdasarkan nilai koefisien secara menurun
coef_df_sorted = coef_df.sort_values(by='Coefficient', ascending=False)

# Plot koefisien yang telah diurutkan
plt.figure(figsize=(12, 6))
sns.barplot(x=coef_df_sorted['Coefficient'], y=coef_df_sorted['Feature'])
plt.title('Logistic Regression Coefficients')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.show()

# Membuat model model machine learning
model_logic_balanced = LogisticRegression(random_state=42, class_weight='balanced')

# mentraining model machine learning
model_logic_balanced.fit(X_train, y_train)

# Melakukan prediksi pada variabel training dan testing
y_train_pred_logic =model_logic_balanced.predict(X_train)
y_test_pred_logic = model_logic_balanced.predict(X_test)

# Melihat hasil akurasi prediksi
print("Akurasi training: ", accuracy_score(y_train, y_train_pred_logic))
print("Akurasi Uji: ", accuracy_score(y_test, y_test_pred_logic))
print(classification_report(y_test, y_test_pred_logic))

# Buat matriks konfusi dari hasil prediksi testing model machine learning
cm_logic = confusion_matrix(y_test, y_test_pred_logic)
plt.figure(figsize=(6, 6))
sns.heatmap(cm_logic, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Membuat DataFrame untuk menyimpan koefisien dan nama fitur
coef_df = pd.DataFrame({'Feature': features_scaled.columns, 'Coefficient': abs(model_logic_balanced.coef_[0])})

# Mengurutkan DataFrame berdasarkan nilai koefisien secara menurun
coef_df_sorted = coef_df.sort_values(by='Coefficient', ascending=False)

# Plot koefisien yang telah diurutkan
plt.figure(figsize=(12, 6))
sns.barplot(x=coef_df_sorted['Coefficient'], y=coef_df_sorted['Feature'])
plt.title('Logistic Regression Coefficients')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.show()

"""**Analisis:**

**Menggunakan `class_weight`:**

- *Kelebihan:* Recall untuk kelas minoritas (1) lebih tinggi (75%), sehingga model cenderung lebih baik dalam mengidentifikasi instansi dari kelas minoritas. Hal ini berguna jika fokus utama adalah mendeteksi kasus positif (1).
- *Keterbatasan:* Precision untuk kelas minoritas (1) relatif rendah (51%), yang berarti ada kemungkinan lebih banyak hasil positif palsu.

**Tidak Menggunakan `class_weight`:**

- *Kelebihan:* Precision untuk kelas minoritas (1) meningkat menjadi 70%, yang berarti ada lebih sedikit hasil positif palsu. Akurasi umum juga sedikit lebih tinggi.
- *Keterbatasan:* Recall untuk kelas minoritas (1) lebih rendah (48%), yang berarti model mungkin melewatkan beberapa kasus positif.

**Kesimpulan:**

- Prioritas dalam kebutuhan ini adalah mengidentifikasi sebanyak mungkin kasus positif `'YES'` atau `'1'` (recall yang tinggi), maka menggunakan `class_weight` mungkin merupakan pilihan yang lebih baik.

## 8.3 Make a New Model Machine Learning SVM

1. **Model SVM Normal:**
   - Membuat model SVM (`model_svm_normal`) dengan kernel linear tanpa menyertakan parameter kelas berimbang.
   - Melatih model menggunakan data latih (`X_train`, `y_train`).
   - Melakukan prediksi pada data latih dan data uji.
   - Menghitung dan menampilkan akurasi serta laporan klasifikasi.
   - Membuat matriks konfusi untuk hasil prediksi pada data uji dan menampilkannya sebagai heatmap.

2. **Model SVM dengan Class Weight 'Balanced':**
   - Membuat model SVM (`model_svm_balanced`) dengan kernel linear dan menyertakan parameter `class_weight='balanced'`.
   - Melatih model menggunakan data latih (`X_train`, `y_train`).
   - Melakukan prediksi pada data latih dan data uji.
   - Menghitung dan menampilkan akurasi serta laporan klasifikasi.
   - Membuat matriks konfusi untuk hasil prediksi pada data uji dan menampilkannya sebagai heatmap.
"""

# Membuat model model machine learning
model_svm_normal = SVC(kernel='linear', C=1.0, random_state=42, gamma=0.1)

# Menampilkan jumlah sampel sebelum oversampling
print("Jumlah sampel sebelum oversampling:", len(X_train))

# Melakukan oversampling dengan SMOTE
smote = SMOTE(random_state=42)
X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)

# Menampilkan jumlah sampel setelah oversampling
print("Jumlah sampel setelah oversampling:", len(X_train_oversampled))


# Mentraining model machine learning
model_svm_normal.fit(X_train_oversampled, y_train_oversampled)

# Melakukan prediksi pada variabel training dan testing
y_train_pred_svm = model_svm_normal.predict(X_train)
y_test_pred_svm = model_svm_normal.predict(X_test)

# Melihat hasil akurasi prediksi
print("Akurasi training: ", accuracy_score(y_train, y_train_pred_svm))
print("Akurasi Uji: ", accuracy_score(y_test, y_test_pred_svm))
print(classification_report(y_test, y_test_pred_svm))

# Buat matriks konfusi dari hasil prediksi testing model machine learning
cm_svm = confusion_matrix(y_test, y_test_pred_svm)
plt.figure(figsize=(6, 6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Membuat DataFrame untuk menyimpan koefisien dan nama fitur
coef_df = pd.DataFrame({'Feature': features_scaled.columns, 'Coefficient': abs(model_svm_normal.coef_[0])})

# Mengurutkan DataFrame berdasarkan nilai koefisien secara menurun
coef_df_sorted = coef_df.sort_values(by='Coefficient', ascending=False)

# Plot koefisien yang telah diurutkan
plt.figure(figsize=(12, 6))
sns.barplot(x=coef_df_sorted['Coefficient'], y=coef_df_sorted['Feature'])
plt.title('Linear SVM Coefficients (Sorted)')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.show()

# Membuat model model machine learning
model_svm_balanced = SVC(kernel='linear', C=1.0, random_state=42, class_weight='balanced', gamma=0.1)

# Mentraining model machine learning
model_svm_balanced.fit(X_train, y_train)

# Melakukan prediksi pada variabel training dan testing
y_train_pred_svm = model_svm_balanced.predict(X_train)
y_test_pred_svm = model_svm_balanced.predict(X_test)

# Melihat hasil akurasi prediksi
print("Akurasi training: ", accuracy_score(y_train, y_train_pred_svm))
print("Akurasi Uji: ", accuracy_score(y_test, y_test_pred_svm))
print(classification_report(y_test, y_test_pred_svm))

# Buat matriks konfusi dari hasil prediksi testing model machine learning
cm_svm = confusion_matrix(y_test, y_test_pred_svm)
plt.figure(figsize=(6, 6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Membuat DataFrame untuk menyimpan koefisien dan nama fitur
coef_df = pd.DataFrame({'Feature': features_scaled.columns, 'Coefficient': abs(model_svm_balanced.coef_[0])})

# Mengurutkan DataFrame berdasarkan nilai koefisien secara menurun
coef_df_sorted = coef_df.sort_values(by='Coefficient', ascending=False)

# Plot koefisien yang telah diurutkan
plt.figure(figsize=(12, 6))
sns.barplot(x=coef_df_sorted['Coefficient'], y=coef_df_sorted['Feature'])
plt.title('Linear SVM Coefficients (Sorted)')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.show()

"""**Analisis:**

**Menggunakan `class_weight`:**

- *Kelebihan:* Recall untuk kelas minoritas (1) lebih tinggi (75%), sehingga model cenderung lebih baik dalam mengidentifikasi instansi dari kelas minoritas. Hal ini berguna jika fokus utama adalah mendeteksi kasus positif (1).
- *Keterbatasan:* Precision untuk kelas minoritas (1) relatif rendah (52%), yang berarti ada kemungkinan lebih banyak hasil positif palsu.

**Tidak Menggunakan `class_weight`:**

- *Kelebihan:* Precision untuk kelas minoritas (1) meningkat menjadi 72%, yang berarti ada lebih sedikit hasil positif palsu. Akurasi umum juga sedikit lebih tinggi.
- *Keterbatasan:* Recall untuk kelas minoritas (1) lebih rendah (45%), yang berarti model mungkin melewatkan beberapa kasus positif.

**Kesimpulan:**

- Prioritas dalam kebutuhan ini adalah mengidentifikasi sebanyak mungkin kasus positif `'YES'` atau `'1'` (recall yang tinggi), maka menggunakan `class_weight` mungkin merupakan pilihan yang lebih baik.

# 9. MODEL INFERENCE

1. DataFrame df_new_data dibuat dengan 10 baris data acak untuk memperoleh hasil prediksi.
2. Fitur baru diubah skala menggunakan fiture_scalar yang sebelumnya telah diinisialisasi.
3. Prediksi dilakukan pada fitur baru menggunakan model logistic regression dengan penanganan ketidakseimbangan (balanced) dan model logistic regression normal.
4. Prediksi juga dilakukan pada fitur baru menggunakan model SVM dengan penanganan ketidakseimbangan (balanced) dan model SVM normal.
"""

fiture.head(10)

print(df_ho['Rainfall'].max())
print(df['Rainfall'].max())

# Membuat DataFrame dengan 10 baris data
data = {
    'Location': np.random.randint(0, 50, 50),
    'Humidity3pm': np.random.randint(0, 100, 50),
    'Humidity9am': np.random.randint(0, 100, 50),
    'Pressure9am': np.random.uniform(900, 1100, 50).round(2),
    'Pressure3pm': np.random.uniform(900, 1100, 50).round(2),
    'Cloud3pm': np.random.randint(0, 9, 50),
    'WindGustSpeed': np.random.randint(0, 150, 50),
    'WindSpeed3pm': np.random.randint(0, 100, 50),
    'Rainfall': np.random.uniform(0, 5, 50).round(2),
    'Temp3pm': np.random.uniform(-10, 50, 50).round(2),
    'RainToday': np.random.choice([1, 0], 50)
}

df_new_data = pd.DataFrame(data)
df_new_data.head()

# Mendefinisikan kolom untuk scaling
columns_to_scale = ['Humidity3pm', 'Humidity9am', 'Pressure9am', 'Pressure3pm', 'Cloud3pm',
                    'WindGustSpeed', 'WindSpeed3pm', 'Rainfall', 'Temp3pm']

# Memisahkan kolom 'Location'
location_column = df_new_data['Location']
rain_column = df_new_data['RainToday']
df_new_data = df_new_data.drop(['Location', 'RainToday'], axis=1)

# Melakukan scaling pada kolom-kolom yang ditentukan
df_scaled = fiture_scalar.transform(df_new_data)

# Membuat DataFrame hasil scaling
features_scaled = pd.DataFrame(df_scaled, columns=columns_to_scale)

# Menambahkan kembali kolom 'Location'
features_scaled['Location'] = location_column.values

# Menambahkan kembali kolom 'Location'
features_scaled['RainToday'] = rain_column.values

# Menampilkan 1 baris features_scaled
features_scaled.sample()

# Melakukan prediksi pada fiture baru menggunakan balace logistic regression
predict_new_fiture = model_logic_balanced.predict(features_scaled)

# Melihat hasil dari fiture baru
predict_new_fiture

# Melakukan prediksi pada fiture baru menggunakan normal logistic regression
predict_new_fiture = model_logic_normal.predict(features_scaled)

# Melihat hasil dari fiture baru
predict_new_fiture

# Melakukan prediksi pada fiture baru menggunakan balace SVM
predict_new_fiture = model_svm_balanced.predict(features_scaled)

# Melihat hasil dari fiture baru
predict_new_fiture

# Melakukan prediksi pada fiture baru menggunakan balace SVM
predict_new_fiture = model_svm_normal.predict(features_scaled)

# Melihat hasil dari fiture baru
predict_new_fiture

"""# 10. PENGAMBILAN KESIMPULAN

# Kesimpulan

Dalam menjalankan proses analisis data cuaca, beberapa langkah penting telah diambil:

1. **Handling Missing Values:**
   - Dilakukan pengecekan dan penanganan nilai yang hilang pada kolom-kolom tertentu, seperti menggunakan imputasi dengan mean, median, atau modus.

   - **Distribusi Data:**
      - Dilakukan analisis distribusi data untuk setiap kolom numerik, mengidentifikasi kolom dengan distribusi normal dan skewness.

   - **Imputasi Data:**
      - Data yang hilang pada kolom-kolom numerik diimputasi dengan menggunakan mean untuk kolom dengan distribusi normal dan median untuk kolom dengan distribusi skew.

2. **Explorasi Data:**
   - Data cuaca memiliki berbagai kolom yang mencakup informasi tentang temperatur, curah hujan, kecepatan angin, dan parameter cuaca lainnya.

   - **Data Visualization:**
      - Dibuat diagram distribusi untuk kolom-kolom numerik dan dilakukan visualisasi skor skewness.

3. **Outlier Handling:**
   - Untuk menghandle outlier menggunakan winsorizer dengan foldnya 3.
   - Yang bertujuan untuk menjaga tetap adanya outlier dengan batas tertentu.
   - Yang bermaksud data cuaca memiliki rentan data yang mempunyai kelonjakan atau perubahan secara ekstrem contohnya seperti badai.

4. **Data Encoding:**
   - Data dibuat menjadi numerik menggunakan `LabelEncoder`.

5. **Data Normalisasi:**
   - Sebelum data akan dinormalisi data akan dipisah menjadi fiture dan target.
   - Fiture dan target akan di normalisasi menggunakan `Robust Scalar`.

6. **Model Machine Learning:**
   - Sebelumnya data akan dipisah menggunakan `train_test_split`.
   - Dilakukan pemilihan dan pelatihan model machine learning, seperti menggunakan Logistic Regression dan SVM.

7. **Analisis Model:**
   - Evaluasi hasil model menggunakan metrik-metrik seperti akurasi, precision, recall, dan f1-score.

   - **Penanganan Ketidakseimbangan Kelas:**
      - Jika terdapat ketidakseimbangan kelas, dilakukan strategi seperti pemberian bobot kelas yaitu `class_weight`.

9. **Model Inference:**
   - Dibuat 10 baris data baru untuk fitur-fitur tertentu dalam bentuk DataFrame.
   - Data baru tersebut akan dinormalisasi menggunakan `scaled_fiture` yang menyimpan model `Robust Scalar`.
   - Memprediksi data baru menggunakan semua model `logistic regression` dan `svm` yang menggunakan class_weight ataupun tidak.

Proses analisis data cuaca ini bertujuan untuk memahami, membersihkan, dan mempersiapkan data untuk pemodelan machine learning serta mengambil langkah-langkah untuk meningkatkan kinerja model terdahap target yang tidak balance.

### 10.1 Export for Deploy

Dari kesimpulan di atas, akan mengekspor beberapa model yang akan di gunakan untuk deploy menggunakan flask, yaitu:
- `fiture_scalar` = untuk menormalisasi data menggunakan `Robust Scalar`.
- `model_svm_balanced` = untuk memprediksi apakah terjadinya hujan untuk esok menggunakan algoritma `SVM` dengan class_weightnya `balanced`.
"""

#save new machine learning model
with open('model_svm.pkl', 'wb') as model_file:
    pickle.dump(model_svm_balanced, model_file)

#save Robust Scalar model
with open('feature_scalar.pkl', 'wb') as model_file:
    pickle.dump(fiture_scalar, model_file)